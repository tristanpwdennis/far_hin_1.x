{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511396c7-00bf-43a0-a016-bd7e5711c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import xarray as xr\n",
    "import sgkit as sg\n",
    "import pandas as pd\n",
    "import numcodecs\n",
    "from Bio import SeqIO\n",
    "from fsspec.implementations.zip import ZipFileSystem\n",
    "import dask\n",
    "import dask.array as da\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a7e813-bcc9-4a61-8fd3-ca437f0a7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get started\n",
    "# Load paths and metadata\n",
    "ref_path = '/QRISdata/Q6151/dennistpw/far_hin_1.x/data/reference/VectorBase-54_AfarautiFAR1_Genome.fasta'\n",
    "output_dir_path = f'/scratch/user/uqtdenni/snp_genotypes_combined.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c114951d-c543-4d9d-ac74-7712ba08b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_contigs(ref_path, length_threshold=1e5):\n",
    "    # now let's get a list of the contigs that we are going to call over\n",
    "    contig_lengths = {}\n",
    "    for record in SeqIO.parse(ref_path, \"fasta\"):\n",
    "        seq_id = record.id\n",
    "        seq_length = len(record.seq)\n",
    "        contig_lengths[seq_id] = seq_length\n",
    "    \n",
    "    # Filter to called contigs (over 1e5 bp)\n",
    "    return {k: v for k, v in sorted(contig_lengths.items(), key=lambda item: item[1], reverse=True) if v > length_threshold}\n",
    "\n",
    "def load_samples():\n",
    "    \n",
    "    ## Returns the sorted unique list of samples from the sample manifest\n",
    "\n",
    "    # Read the samples from the file\n",
    "    fofn = pd.read_table('/home/uqtdenni/far_hin_1.x/metadata/fofn.tsv')\n",
    "\n",
    "    # Get the unique list of samples from the DataFrame and sort\n",
    "    samples = fofn['zarr_name'].dropna().unique().tolist()\n",
    "    return samples\n",
    "\n",
    "#called_contigs = get_contigs(ref_path)\n",
    "#samples = load_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd053ca-fee0-4a87-8757-fb5eeb2b459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "called_contigs = get_contigs(ref_path)\n",
    "samples = load_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae49f1-ebe7-45e0-ace6-7909960fe3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing contig: KI915067\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def open_sample_contig(sample, contig):\n",
    "    zip_file = f\"/scratch/user/uqtdenni/snp_genotypes_zarr/{sample}.zarr.zip\"\n",
    "    \n",
    "    zip_fs = ZipFileSystem(zip_file)\n",
    "\n",
    "    # Map the full path to one contig\n",
    "    contig_path = f\"{sample}.zarr/{contig}\"\n",
    "    \n",
    "    zarr_store = zip_fs.get_mapper(contig_path)\n",
    "\n",
    "    z = xr.open_zarr(zarr_store, consolidated=False)\n",
    "\n",
    "    \n",
    "    return z\n",
    "\n",
    "\n",
    "def rechunk_dataset(ds, max_chunk_size=100_000):\n",
    "    \"\"\"\n",
    "    Rechunk all data variables in an xarray.Dataset so that\n",
    "    chunks along the 'variants' dimension are <= max_chunk_size,\n",
    "    and samples/ploidy chunks stay reasonable.\n",
    "    \n",
    "    Args:\n",
    "        ds (xr.Dataset): Input dataset\n",
    "        max_chunk_size (int): max chunk size for variants dimension\n",
    "    \n",
    "    Returns:\n",
    "        xr.Dataset: Rechunked dataset\n",
    "    \"\"\"\n",
    "    rechunked_vars = {}\n",
    "    for var in ds.data_vars:\n",
    "        arr = ds[var]\n",
    "        chunks = arr.chunks\n",
    "        \n",
    "        # If no chunks info (not dask array), skip rechunk\n",
    "        if chunks is None:\n",
    "            rechunked_vars[var] = arr\n",
    "            continue\n",
    "        \n",
    "        dims = arr.dims\n",
    "        \n",
    "        # Default: keep all chunks except 'variants' dimension,\n",
    "        # rechunk variants to max_chunk_size (or less if smaller)\n",
    "        chunk_sizes = []\n",
    "        for dim in dims:\n",
    "            if dim == \"variants\":\n",
    "                # set chunk size along variants dimension\n",
    "                chunk_sizes.append(min(max_chunk_size, arr.sizes[dim]))\n",
    "            else:\n",
    "                # keep current chunk size or whole dimension if chunk info missing\n",
    "                dim_idx = dims.index(dim)\n",
    "                try:\n",
    "                    chunk_sizes.append(chunks[dim_idx][0])\n",
    "                except:\n",
    "                    chunk_sizes.append(arr.sizes[dim])\n",
    "        \n",
    "        # Apply rechunk via dask\n",
    "        rechunked_vars[var] = arr.chunk(dict(zip(dims, chunk_sizes)))\n",
    "    \n",
    "    return ds.update(rechunked_vars)\n",
    "\n",
    "# The field you want to merge (example)\n",
    "\n",
    "# Dictionary to hold merged contig arrays\n",
    "merged_contigs = {}\n",
    "\n",
    "for contig in ['KI915067']:\n",
    "    print(f\"Processing contig: {contig}\")\n",
    "    arrays = []\n",
    "    for sample in samples:\n",
    "        ds = open_sample_contig(sample, contig)  # your existing function\n",
    "        arrays.append(ds)\n",
    "\n",
    "    # Combine samples (dim='samples') across datasets\n",
    "    merged_ds = xr.concat(arrays, dim=\"samples\")\n",
    "\n",
    "    # Fix dims if needed\n",
    "    merged_ds = merged_ds.transpose(\"variants\", \"samples\", ...)\n",
    "\n",
    "    # Drop unwanted vars, convert ids to str\n",
    "    # Drop unwanted variables\n",
    "    merged_ds = merged_ds.drop_vars(['filter_id', 'variant_id','variant_id_mask','call_genotype_mask','call_genotype_phased','variant_id_mask','variant_quality','contig_length', 'contig_id','variant_filter','variant_contig'])\n",
    "\n",
    "        # Calculate allele lengths (assumes variant_allele is an array of strings)\n",
    "    allele_lengths = merged_ds['variant_allele'].astype(str).str.len()\n",
    "    \n",
    "    # Create boolean mask: True for indels (length > 1)\n",
    "    is_indel = allele_lengths > 1\n",
    "    \n",
    "    # Variables representing genotype calls to mask â€” adjust as needed\n",
    "    genotype_vars = ['call_genotype', 'variant_allele', 'call_AD', 'call_GQ','variant_MQ']\n",
    "    \n",
    "    for var in genotype_vars:\n",
    "        if var in merged_ds:\n",
    "            # Set genotype calls to -1 for indels, keep others as is\n",
    "            merged_ds[var] = merged_ds[var].where(~is_indel, other=-1)\n",
    "\n",
    "    # Rechunk \n",
    "    new_chunks = {'variants': 10000, 'samples': 10}\n",
    "\n",
    "    for var_name, da in merged_ds.data_vars.items():\n",
    "        # Only keep keys in new_chunks that are actually in the dims of this variable\n",
    "        chunks_for_var = {dim: size for dim, size in new_chunks.items() if dim in da.dims}\n",
    "        merged_ds[var_name] = da.chunk(chunks_for_var)\n",
    "        #print(f\"{var_name} chunks: {merged_ds[var_name].chunks}\") \n",
    "\n",
    "        \n",
    "    # Convert object dtype variables to string dtype\n",
    "    for var in ['sample_id', 'variant_allele']:\n",
    "        if var in merged_ds:\n",
    "            merged_ds[var] = merged_ds[var].astype(str)\n",
    "    \n",
    "#    # Save the dataset\n",
    "    output_dir = f\"/scratch/user/uqtdenni/merged_zarr/{contig}.zarr\"\n",
    "    if os.path.exists(output_dir):\n",
    "        import shutil\n",
    "        shutil.rmtree(output_dir)\n",
    "\n",
    "    merged_ds.to_zarr(output_dir, encoding={k: {} for k in merged_ds.data_vars})\n",
    "    print(f\"Saved merged dataset for {contig} to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "far_hin_1.x",
   "language": "python",
   "name": "far_hin_1.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
