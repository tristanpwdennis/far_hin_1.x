{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20b1e67-b39c-4de7-bb79-cb40bbad5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import zarr\n",
    "import bed_reader\n",
    "import allel\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a13fc11-290f-4f85-b33f-b15158bff360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper funcs\n",
    "# Define metadata and qc bool globally to start\n",
    "#define useful variables\n",
    "zarr_base_path = f\"/scratch/user/uqtdenni/afar_production_bunya/curation/uq-beebe-001/combined_zarr/{{contig}}.zarr\"\n",
    "\n",
    "# Let's start by converting zarrs for the 5 largest contigs - we can do QC on them...\n",
    "ref_path = '/scratch/user/uqtdenni/afar_production_bunya/reference/VectorBase-54_AfarautiFAR1_Genome.fasta'\n",
    "# now let's get a list of the contigs that we are going to call over\n",
    "contig_lengths = {}\n",
    "for record in SeqIO.parse(ref_path, \"fasta\"):\n",
    "    seq_id = record.id\n",
    "    seq_length = len(record.seq)\n",
    "    contig_lengths[seq_id] = seq_length\n",
    "filtered_contigs = {k: v for k, v in sorted(contig_lengths.items(), key=lambda item: item[1], reverse=True) if v > 100000}\n",
    "\n",
    "# Because these data are unstaged, we need to faff about a bit more and load the unstaged metadata to exclude extra dud samples\n",
    "# And load the final (cleaned) metadata\n",
    "df_samples = pd.read_csv('/scratch/user/uqtdenni/far_hin_1.x/work/metadata_development_20250702/metadata-staged-speciesconfirmed-20251011.txt',sep='\\t')\n",
    "\n",
    "\n",
    "# Zarr location\n",
    "zarr_base_path = f\"/scratch/user/uqtdenni/afar_production_bunya/curation/uq-beebe-001/staged_zarr/{{contig}}.zarr\"\n",
    "\n",
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a2d8ef-598c-4c52-8e93-daf2610bd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_params(*args, **kwargs):\n",
    "    \"\"\"Helper function to hash analysis parameters.\"\"\"\n",
    "    o = {\n",
    "        'args': args,\n",
    "        'kwargs': kwargs\n",
    "    }\n",
    "    s = json.dumps(o, sort_keys=True).encode()\n",
    "    h = hashlib.md5(s).hexdigest()\n",
    "    return h\n",
    "\n",
    "# Define helper functions\n",
    "def load_genotype_array(contig, df_samples=df_samples, sample_query = None, n_snps=None):\n",
    "    # Load gts and remove failed qc samples\n",
    "    z = zarr.open(zarr_base_path.format(contig=contig))\n",
    "    \n",
    "    # Variant-level mask: punctulatus_group_filter_pass\n",
    "    filter_mask = z[f\"{contig}/filter_pass\"][:]\n",
    "    \n",
    "    # Apply combined variant mask\n",
    "    gt = allel.GenotypeChunkedArray(z[f\"calldata/GT\"])\n",
    "    gt = gt.compress(filter_mask, axis=0)    # Filter variants\n",
    "\n",
    "    # If an additional mask is supplied to subset the data from the finished metadata, apply, else return all samples\n",
    "    if sample_query is not None:\n",
    "        bool_query = np.array(df_samples.eval(sample_query))\n",
    "        gt = gt.compress(bool_query, axis=1)\n",
    "    if n_snps is not None:\n",
    "            gt = select_random_genotypes_sorted(gt, n_snps)\n",
    "\n",
    "    return gt\n",
    "\n",
    "\n",
    "def select_random_elements_sorted(g, x, replace=False, seed=None):\n",
    "    \"\"\"\n",
    "    Select x random rows from a 2D array, returned in sorted order.\n",
    "\n",
    "    Parameters:\n",
    "    - array: array 2d, shape (n_genotypes, n_features)\n",
    "    - x: int, number of rows to select\n",
    "    - replace: bool, whether sampling is with replacement (default: False)\n",
    "    - seed: int, random seed for reproducibility (default: None)\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray of shape (x, n_features)\n",
    "    \"\"\"\n",
    "\n",
    "    # Select random sites from that set\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_rows = g.shape[0]\n",
    "    if not replace and x > n_rows:\n",
    "        raise ValueError(f\"Cannot select {x} rows without replacement from {n_rows} total rows.\")\n",
    "    indices = rng.choice(n_rows, size=x, replace=replace)\n",
    "    sorted_indices = np.sort(indices)\n",
    "\n",
    "    return g[sorted_indices]\n",
    "\n",
    "def compute_ac(contig, is_biallelic=True, is_segregating=True, min_minor_ac=1, n_snps=None, sample_query=None, to_alt = None):\n",
    "    \n",
    "    g = load_genotype_array(contig=contig, sample_query=sample_query)  \n",
    "    \n",
    "    ac = g.count_alleles()\n",
    "    \n",
    "    mask = None\n",
    "    \n",
    "    # Apply biallelic filter\n",
    "    if is_biallelic:\n",
    "        biallelic_mask = ac.is_biallelic()\n",
    "        mask = biallelic_mask if mask is None else mask & biallelic_mask\n",
    "    \n",
    "    # Apply segregating filter\n",
    "    if is_segregating:\n",
    "        segregating_mask = ac.is_segregating()\n",
    "        mask = segregating_mask if mask is None else mask & segregating_mask\n",
    "    \n",
    "    # Apply minor allele count filter\n",
    "    if min_minor_ac is not None:\n",
    "        an = ac.sum(axis=1)\n",
    "    # Apply minor allele count condition.\n",
    "        ac_minor = ac[:, 1:].sum(axis=1)\n",
    "        if isinstance(min_minor_ac, float):\n",
    "            ac_minor_frac = ac_minor / an\n",
    "            loc_minor_mask = ac_minor_frac >= min_minor_ac\n",
    "        else:\n",
    "            loc_minor_mask = ac_minor >= min_minor_ac\n",
    "        mask = loc_minor_mask if mask is None else mask & loc_minor_mask\n",
    "    \n",
    "    # Apply all filters at once\n",
    "    if mask is not None:\n",
    "        gt = g.compress(mask)\n",
    "    \n",
    "    # Random selection (if needed)\n",
    "    if n_snps is not None:  # Fixed: 'if' instead of 'is'\n",
    "        gt = select_random_elements_sorted(gt, n_snps)\n",
    "    \n",
    "    if to_alt is not None:\n",
    "        return gt.to_n_alt()\n",
    "    else:\n",
    "        return gt.count_alleles()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4122798a-61b9-4c6c-9477-f68cba8f9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = 'species_pca == \"hinesorum\"'\n",
    "contig = \"KI915040\"\n",
    "n_snps = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4df2ce8f-79fc-4538-bddd-f84f9b53622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sites: 7175601\n",
      "After initial filters: 3761669\n",
      "After truly biallelic filter: 3660986\n"
     ]
    }
   ],
   "source": [
    "# Load data, set some filters (filter mask pass, variable, biallelic, not a singleston)\n",
    "x = zarr.open(zarr_base_path.format(contig=contig))\n",
    "g = allel.GenotypeArray(x['calldata/GT'])\n",
    "\n",
    "# Start making filters over the whole dataset (e.g. allelism)\n",
    "ac = g.count_alleles() #Count alleles\n",
    "\n",
    "# Initial filters\n",
    "flt = ac.is_biallelic() & (ac.max_allele() == 1)\n",
    "filter_mask = x['variants/filter_pass']\n",
    "has_asterisk = ~np.any(x['variants/ALT'][:] == '*', axis=1)\n",
    "\n",
    "# Combine initial filters\n",
    "initial_mask = flt & filter_mask & has_asterisk\n",
    "\n",
    "# Apply initial mask to ALT array to check for truly biallelic sites\n",
    "alt_temp = x['variants/ALT'][:][initial_mask]\n",
    "truly_biallelic_subset = np.sum(alt_temp != '', axis=1) == 1\n",
    "\n",
    "# Create final mask by combining initial mask with truly biallelic filter\n",
    "final_mask = np.zeros(len(initial_mask), dtype=bool)\n",
    "final_mask[initial_mask] = truly_biallelic_subset\n",
    "\n",
    "print(f\"Original sites: {len(initial_mask)}\")\n",
    "print(f\"After initial filters: {np.sum(initial_mask)}\")\n",
    "print(f\"After truly biallelic filter: {np.sum(final_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c27a4d2f-6c8b-4813-9ba3-034cb81a5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to farauti_ss inds passing filters\n",
    "#far_inds = df_samples.eval(sample_query)\n",
    "g_f = g#.compress(far_inds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d231df0-d870-4b7d-a1d3-00c1c4de1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble everything\n",
    "gt = select_random_elements_sorted(g_f.compress(final_mask), 100_000, seed=1234)\n",
    "c = select_random_elements_sorted(x['variants/CHROM'][:].compress(final_mask), 100_000, seed=1234)\n",
    "pos = select_random_elements_sorted(x['variants/POS'][:].compress(final_mask), 100_000, seed=1234)\n",
    "ref = select_random_elements_sorted(x['variants/REF'][:].compress(final_mask), 100_000, seed=1234)\n",
    "alt = select_random_elements_sorted(x['variants/ALT'][:][final_mask], 100_000, seed=1234)[:, 0]\n",
    "\n",
    "gn = gt.to_n_alt().T\n",
    "iid = list(df_samples.index)\n",
    "\n",
    "\n",
    "properties = {\n",
    "        \"iid\": iid,\n",
    "        \"chromosome\": np.ones(100000, dtype=int), # just ones as admix kicks off if you have text as chrom ids\n",
    "        \"bp_position\": pos.astype(int),\n",
    "        \"allele_1\": ref,\n",
    "        \"allele_2\":alt,\n",
    "    }\n",
    "\n",
    "bed_reader.to_bed(\n",
    "    filepath='all_samples_1e5.bed',\n",
    "    val=gn,\n",
    "    properties=properties,\n",
    "    count_A1=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "far_hin_1.x",
   "language": "python",
   "name": "far_hin_1.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
