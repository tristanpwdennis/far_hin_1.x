{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e426367-fc1f-414e-9aba-238bf4e7cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import pickle\n",
    "import allel\n",
    "import seaborn as sns\n",
    "import zarr\n",
    "import plotly.express as px\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from numpy.random import binomial\n",
    "from Bio import SeqIO\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ca55ef-a18c-4665-8776-5bf998235d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata and qc bool globally to start\n",
    "#define useful variables\n",
    "zarr_base_path = f\"/scratch/user/uqtdenni/afar_production_bunya/curation/uq-beebe-001/combined_zarr/{{contig}}.zarr\"\n",
    "\n",
    "# Let's start by converting zarrs for the 5 largest contigs - we can do QC on them...\n",
    "ref_path = '/scratch/user/uqtdenni/afar_production_bunya/reference/VectorBase-54_AfarautiFAR1_Genome.fasta'\n",
    "# now let's get a list of the contigs that we are going to call over\n",
    "contig_lengths = {}\n",
    "for record in SeqIO.parse(ref_path, \"fasta\"):\n",
    "    seq_id = record.id\n",
    "    seq_length = len(record.seq)\n",
    "    contig_lengths[seq_id] = seq_length\n",
    "filtered_contigs = {k: v for k, v in sorted(contig_lengths.items(), key=lambda item: item[1], reverse=True) if v > 100000}\n",
    "\n",
    "# Because these data are unstaged, we need to faff about a bit more and load the unstaged metadata to exclude extra dud samples\n",
    "df_samples_dirty = pd.read_csv('/scratch/user/uqtdenni/far_hin_1.x/work/metadata_development_20250702/sample_metadata_interim_seq_qc_pass.txt', index_col = 'derived_sample_id')\n",
    "# And load the final (cleaned) metadata\n",
    "df_samples = pd.read_csv('/scratch/user/uqtdenni/far_hin_1.x/work/metadata_development_20250702/sample_metadata_pass_qc.txt', index_col = 'derived_sample_id')\n",
    "\n",
    "# Mask removing samples we removed before the staging step of QC (that I haven't done yet)\n",
    "qc_bool = df_samples_dirty.index.isin(df_samples.index)\n",
    "\n",
    "# Zarr location\n",
    "zarr_base_path = f\"/scratch/user/uqtdenni/afar_production_bunya/curation/uq-beebe-001/combined_zarr/{{contig}}.zarr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c950974-f703-46ec-85a9-d89310be1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def load_genotype_array(contig, qc_bool=qc_bool, df_samples=df_samples, sample_query = None):\n",
    "    # Load gts and remove failed qc samples\n",
    "    z = zarr.open(zarr_base_path.format(contig=contig))\n",
    "    \n",
    "    # Variant-level mask: punctulatus_group_filter_pass\n",
    "    filter_mask = z[f\"{contig}/punctulatus_group_filter_pass\"][:]\n",
    "\n",
    "    gt = allel.GenotypeChunkedArray(z[f\"{contig}/calldata/GT\"])\n",
    "    \n",
    "    # Apply combined variant mask\n",
    "    gt = allel.GenotypeChunkedArray(z[f\"{contig}/calldata/GT\"])\n",
    "    gt = gt.compress(qc_bool, axis=1)          # Filter samples by QC\n",
    "    gt = gt.compress(filter_mask, axis=0)    # Filter variants\n",
    "    \n",
    "    # If an additional mask is supplied to subset the data from the finished metadata, apply, else return all samples\n",
    "    if sample_query is not None:\n",
    "        bool_query = np.array(df_samples.eval(sample_query))\n",
    "        return gt.compress(bool_query, axis=1)\n",
    "    else:\n",
    "        return gt\n",
    "\n",
    "def collapse_ac_array(biallelic_ac_array):\n",
    "    #ONLY EVER USE THIS IF YOU KNOW YOUR AC ARRAY CONTAINS 2 ELEMENTS PER ROW AND MULTIALLELICS HAVE BEEN FILTERED OUT\n",
    "        # Preallocate 2-column array\n",
    "    collapsed_fixed = np.zeros((biallelic_ac_array.shape[0], 2), dtype=biallelic_ac_array.dtype)\n",
    "    \n",
    "    # Fill it with nonzero values\n",
    "    for i, row in enumerate(biallelic_ac_array):\n",
    "        nz = row[row > 0]\n",
    "        collapsed_fixed[i, :len(nz)] = nz\n",
    "    return(collapsed_fixed)\n",
    "\n",
    "def compute_ac(\n",
    "    contig, \n",
    "    qc_bool=qc_bool, \n",
    "    df_samples=df_samples, \n",
    "    segregating=False, \n",
    "    biallelics_only=False, \n",
    "    remove_singletons=False,\n",
    "    sample_query=None, \n",
    "    seg_mask = None, # Provide precomputed seg mask\n",
    "):\n",
    "    # Load genotypes\n",
    "    gt = load_genotype_array(contig, qc_bool, df_samples, sample_query)\n",
    "    ac = gt.count_alleles()\n",
    "\n",
    "    # Filter for segregating sites if requested\n",
    "    if segregating:\n",
    "        ac = ac.compress(ac.is_segregating())\n",
    "    \n",
    "    # Or use a presupplied mask\n",
    "    if seg_mask:\n",
    "        ac = ac.compress(seg_mask)\n",
    "\n",
    "    # Filter for biallelic sites if requested\n",
    "    if biallelics_only:\n",
    "        ac = ac.compress(ac.is_biallelic())\n",
    "\n",
    "    if remove_singletons:\n",
    "        mask_singleton = np.asarray(ac.is_singleton())\n",
    "        ac = ac.compress(~mask_singleton)\n",
    "\n",
    "    return ac\n",
    "\n",
    "# Set up causal snp architecture - is it a single snp with a massive effect size, like in Aedes, or multiple snps with modest effect sizes, or polygenic (aggrgrhgh)\n",
    "def setup_causal_architecture(architecture, maf):\n",
    "    \"\"\"Setup causal SNPs and effects based on architecture type\"\"\"\n",
    "    \n",
    "    # Get SNPs with intermediate frequencies\n",
    "    intermediate_freq_mask = (maf > 0.2) & (maf < 0.5)\n",
    "    available_snps = np.where(intermediate_freq_mask)[0]\n",
    "    \n",
    "    if architecture == \"monogenic\":\n",
    "        # Single SNP with massive effect\n",
    "        n_causal = 1\n",
    "        causal_snp_indices = np.random.choice(available_snps, size=n_causal, replace=False)\n",
    "        causal_effects = np.array([np.log(7)])  # OR = 7, large effect\n",
    "        \n",
    "    elif architecture == \"oligogenic\":\n",
    "        # Few SNPs with moderate effects\n",
    "        n_causal = 4\n",
    "        causal_snp_indices = np.random.choice(available_snps, size=n_causal, replace=False)\n",
    "        causal_effects = np.array([np.log(2.5), np.log(2.0), np.log(1.8)])  # OR = 1.8-2.5\n",
    "        \n",
    "    elif architecture == \"polygenic\":\n",
    "        # Many SNPs with small effects\n",
    "        n_causal = 15\n",
    "        causal_snp_indices = np.random.choice(available_snps, size=n_causal, replace=False)\n",
    "        causal_effects = np.random.normal(np.log(1.2), 0.1, n_causal)  # OR ~1.1-1.3\n",
    "        \n",
    "    return causal_snp_indices, causal_effects\n",
    "\n",
    "# Simulate larger population (assuming HWE)\n",
    "def simulate_genotypes(allele_freqs, n_inds):\n",
    "    n_snps = len(allele_freqs)\n",
    "    genotypes = np.zeros((n_inds, n_snps))\n",
    "    \n",
    "    for i, freq in enumerate(allele_freqs):\n",
    "        # Binomial sampling: 2 alleles per individual\n",
    "        genotypes[:, i] = np.random.binomial(2, freq, n_inds)\n",
    "    \n",
    "    return genotypes\n",
    "\n",
    "def run_gwas(genotypes, phenotype):\n",
    "    \"\"\"Run GWAS with robust handling of sparse contingency tables\"\"\"\n",
    "    n_snps = genotypes.shape[1]\n",
    "    p_values = np.ones(n_snps)  # Default to p=1 (no association)\n",
    "    \n",
    "    for i in range(n_snps):\n",
    "        # Create contingency table\n",
    "        contingency_table = np.array([\n",
    "            [np.sum((genotypes[:, i] == 0) & (phenotype == 0)),  # AA controls\n",
    "             np.sum((genotypes[:, i] == 0) & (phenotype == 1))], # AA cases\n",
    "            [np.sum((genotypes[:, i] == 1) & (phenotype == 0)),  # AB controls  \n",
    "             np.sum((genotypes[:, i] == 1) & (phenotype == 1))], # AB cases\n",
    "            [np.sum((genotypes[:, i] == 2) & (phenotype == 0)),  # BB controls\n",
    "             np.sum((genotypes[:, i] == 2) & (phenotype == 1))]  # BB cases\n",
    "        ])\n",
    "        \n",
    "        # Check for sparse table (zeros in expected frequencies)\n",
    "        row_sums = contingency_table.sum(axis=1)\n",
    "        col_sums = contingency_table.sum(axis=0)\n",
    "        \n",
    "        # Remove rows/cols with all zeros\n",
    "        non_zero_rows = row_sums > 0\n",
    "        non_zero_cols = col_sums > 0\n",
    "        \n",
    "        if np.sum(non_zero_rows) < 2 or np.sum(non_zero_cols) < 2:\n",
    "            # Not enough data for test\n",
    "            continue\n",
    "            \n",
    "        filtered_table = contingency_table[non_zero_rows][:, non_zero_cols]\n",
    "        \n",
    "        try:\n",
    "            chi2, p_val = stats.chi2_contingency(filtered_table)[:2]\n",
    "            p_values[i] = p_val\n",
    "        except ValueError:\n",
    "            # Still problematic - skip this SNP\n",
    "            continue\n",
    "    \n",
    "    return p_values\n",
    "\n",
    "# More realistic approach - simulate from logistic model\n",
    "def simulate_phenotype_logistic(risk_scores, baseline_prob=0.5):\n",
    "    \"\"\"Simulate phenotype using logistic regression\"\"\"\n",
    "    # Adjust intercept to get desired baseline probability\n",
    "    intercept = np.log(baseline_prob / (1 - baseline_prob)) - np.mean(risk_scores)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    log_odds = intercept + risk_scores\n",
    "    probabilities = 1 / (1 + np.exp(-log_odds))\n",
    "    \n",
    "    # Sample binary outcomes\n",
    "    binary_trait = np.random.binomial(1, probabilities)\n",
    "    \n",
    "    return binary_trait\n",
    "\n",
    "def create_balanced_cases_controls(risk_scores, desired_cases):\n",
    "    sorted_indices = np.argsort(risk_scores)[::-1]\n",
    "    binary_trait = np.zeros(len(risk_scores), dtype=int)\n",
    "    binary_trait[sorted_indices[:desired_cases]] = 1\n",
    "    return binary_trait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307482e4-3983-4266-b525-a2f76796cd22",
   "metadata": {},
   "source": [
    "### Simulating GWAS using allele frequencies from a population of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115bfc7-c7d4-41dc-8962-1aacad5f0cca",
   "metadata": {},
   "source": [
    "We are going to simulate our GWAS using simulated genotypes, generated from the SFS of a smaller sequenced population (that we will return to to study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e6dad-d454-4b59-8b8c-bbfded1052af",
   "metadata": {},
   "source": [
    "Let's start by setting up the genotype and allele frequency data. Compute minor allele frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4fdff14-e5ea-44cd-b9b3-8ac4a50fdcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute allele_counts                \n",
    "wp_ac = compute_ac(contig = \"KI915040\", \n",
    "           qc_bool = qc_bool, \n",
    "           df_samples=df_samples, \n",
    "           sample_query = 'species == \"hinesorum\" & admin1_iso == \"SB-WE\"',\n",
    "           segregating=True,\n",
    "           biallelics_only=True,\n",
    "           remove_singletons=True)\n",
    "\n",
    "# convert to frequencies, select maf (eg column 1)\n",
    "wp_maf = wp_ac.to_frequencies()[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007a454-596a-4b74-8a22-833f84797b95",
   "metadata": {},
   "source": [
    "Taking the mafs from the population (Solomon Islands Western Province), let's run a simulated GWAS. For simplicity's sake we'll start with a single SNP with a massive odds ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f08b3f-50d3-454d-b447-664dfc2bcfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment Setup & Debug===\n",
      "Cases: 30, Controls: 70\n",
      "Cases indices: [ 6  9 10 11 15 19 20 25 27 28 30 32 35 37 38 42 43 44 46 49 60 66 67 69\n",
      " 72 76 77 79 80 83]\n",
      "Cases risk scores: [1.94591015 1.94591015 1.94591015 3.8918203  1.94591015 1.94591015\n",
      " 3.8918203  1.94591015 1.94591015 1.94591015 1.94591015 1.94591015\n",
      " 1.94591015 3.8918203  3.8918203  1.94591015 1.94591015 1.94591015\n",
      " 3.8918203  1.94591015 1.94591015 1.94591015 1.94591015 1.94591015\n",
      " 1.94591015 1.94591015 1.94591015 1.94591015 1.94591015 1.94591015]\n",
      "All cases have high risk scores: False\n"
     ]
    }
   ],
   "source": [
    "# Set up - small sample size for now, but massive OR of the SNP so should be ok?\n",
    "n_individuals = 100\n",
    "arch = \"monogenic\"\n",
    "desired_cases = 30 #30 % trait frequency  \n",
    "\n",
    "# Let's extract causal SNPs of an intermediate frequency for a monogenic trait\n",
    "causal_snp_indices, causal_effects = setup_causal_architecture(arch, wp_maf)\n",
    "simulated_genos = simulate_genotypes(wp_maf, n_individuals)\n",
    "\n",
    "# Calculate risk scores \n",
    "risk_scores = np.zeros(n_individuals)\n",
    "for i, snp_idx in enumerate(causal_snp_indices):\n",
    "    risk_scores += causal_effects[i] * simulated_genos[:, snp_idx]\n",
    "\n",
    "# Create balanced case/control split\n",
    "binary_trait = create_balanced_cases_controls(risk_scores, desired_cases)\n",
    "\n",
    "print(\"=== Experiment Setup & Debug===\")\n",
    "print(f\"Cases: {binary_trait.sum()}, Controls: {(1-binary_trait).sum()}\")\n",
    "# 3. Verify immediately after assignment\n",
    "cases_indices = np.where(binary_trait == 1)[0]\n",
    "print(f\"Cases indices: {cases_indices}\")\n",
    "print(f\"Cases risk scores: {risk_scores[cases_indices]}\")\n",
    "print(f\"All cases have high risk scores: {np.all(risk_scores[cases_indices] >= 2.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c49bafc7-4ccf-490a-bb6c-6f80cdcd4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running GWAS, FLY MY PRETTIES, FLY!==\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run GWAS! fly my pretties!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Running GWAS, FLY MY PRETTIES, FLY!==\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m p_values \u001b[38;5;241m=\u001b[39m \u001b[43mrun_gwas\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulated_genos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_trait\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP-value at causal SNP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_values[causal_snp_indices[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m, in \u001b[0;36mrun_gwas\u001b[0;34m(genotypes, phenotype)\u001b[0m\n\u001b[1;32m    108\u001b[0m p_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(n_snps)  \u001b[38;5;66;03m# Default to p=1 (no association)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_snps):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# Create contingency table\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     contingency_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m    113\u001b[0m         [np\u001b[38;5;241m.\u001b[39msum((genotypes[:, i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (phenotype \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)),  \u001b[38;5;66;03m# AA controls\u001b[39;00m\n\u001b[1;32m    114\u001b[0m          np\u001b[38;5;241m.\u001b[39msum((genotypes[:, i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (phenotype \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))], \u001b[38;5;66;03m# AA cases\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m         [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenotypes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mphenotype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# AB controls  \u001b[39;00m\n\u001b[1;32m    116\u001b[0m          np\u001b[38;5;241m.\u001b[39msum((genotypes[:, i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (phenotype \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))], \u001b[38;5;66;03m# AB cases\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         [np\u001b[38;5;241m.\u001b[39msum((genotypes[:, i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m&\u001b[39m (phenotype \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)),  \u001b[38;5;66;03m# BB controls\u001b[39;00m\n\u001b[1;32m    118\u001b[0m          np\u001b[38;5;241m.\u001b[39msum((genotypes[:, i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m&\u001b[39m (phenotype \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))]  \u001b[38;5;66;03m# BB cases\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     ])\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Check for sparse table (zeros in expected frequencies)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     row_sums \u001b[38;5;241m=\u001b[39m contingency_table\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:2344\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2338\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `min` or `max` keyword argument when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2339\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2345\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2351\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run GWAS! fly my pretties!\n",
    "print(\"=== Running GWAS, FLY MY PRETTIES, FLY!==\")\n",
    "p_values = run_gwas(simulated_genos, binary_trait)\n",
    "print(f\"P-value at causal SNP: {p_values[causal_snp_indices[0]]:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43bc2b5-ed8e-4a31-b64c-85083092e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = np.sum(p_values[causal_snp_indices] < 5e-8)\n",
    "power = detected / len(causal_snp_indices)\n",
    "power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76618d3-a3bf-4997-87e9-5b940a494e24",
   "metadata": {},
   "source": [
    "OK, now we've run our simulated GWAS for a monogenic trait, with a large effect. It looks like we have good power with 100 samples to detect this, so let's run some more simulations with some more complex trait architectures, a range of sample and effect sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f5ba9-a90e-40e6-9946-a73b66edda97",
   "metadata": {},
   "source": [
    "## Full power analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796d58a0-270a-4813-94b5-9a5d73ba577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def create_parameter_hash(architecture, trait_freq, effect_size, sample_size, n_replicates):\n",
    "    \"\"\"Create a hash from simulation parameters for unique file naming\"\"\"\n",
    "    params = f\"{architecture}_{trait_freq}_{effect_size}_{sample_size}_{n_replicates}\"\n",
    "    return hashlib.md5(params.encode()).hexdigest()[:8]\n",
    "\n",
    "def check_simulation_exists(output_path, param_hash):\n",
    "    \"\"\"Check if simulation already exists\"\"\"\n",
    "    pickle_file = output_path / f\"sim_{param_hash}.pkl\"\n",
    "    return pickle_file.exists()\n",
    "\n",
    "def load_existing_simulation(output_path, param_hash):\n",
    "    \"\"\"Load existing simulation result\"\"\"\n",
    "    pickle_file = output_path / f\"sim_{param_hash}.pkl\"\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_simulation_result(result, output_path, param_hash):\n",
    "    \"\"\"Save simulation result\"\"\"\n",
    "    pickle_file = output_path / f\"sim_{param_hash}.pkl\"\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    return pickle_file\n",
    "\n",
    "def setup_causal_architecture(architecture_type, allele_freqs, effect_size=None):\n",
    "    \"\"\"Setup causal SNPs and effects based on architecture type\"\"\"\n",
    "    intermediate_freq_mask = (allele_freqs > 0.2) & (allele_freqs < 0.5)\n",
    "    available_snps = np.where(intermediate_freq_mask)[0]\n",
    "    \n",
    "    if architecture_type == \"monogenic\":\n",
    "        n_causal = 1\n",
    "        causal_snp_indices = np.random.choice(available_snps, size=n_causal, replace=False)\n",
    "        or_effect = effect_size if effect_size is not None else 7\n",
    "        causal_effects = np.array([np.log(or_effect)])\n",
    "        \n",
    "    elif architecture_type == \"oligogenic\":\n",
    "        n_causal = 3\n",
    "        causal_snp_indices = np.random.choice(available_snps, size=n_causal, replace=False)\n",
    "        base_effects = [2.5, 2.0, 1.8]\n",
    "        if effect_size is not None:\n",
    "            scale_factor = effect_size / 2.0\n",
    "            base_effects = [e * scale_factor for e in base_effects]\n",
    "        causal_effects = np.array([np.log(e) for e in base_effects])\n",
    "        \n",
    "    return causal_snp_indices, causal_effects\n",
    "\n",
    "def power_analysis(output_dir=\"gwas_results\", trait_frequencies=None, effect_sizes=None, n_replicates=10):\n",
    "    \"\"\"Run power analysis with caching\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Default parameters\n",
    "    if trait_frequencies is None:\n",
    "        trait_frequencies = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    \n",
    "    if effect_sizes is None:\n",
    "        effect_sizes = {\n",
    "            \"monogenic\": [3, 5, 7, 10],\n",
    "            \"oligogenic\": [1.5, 2.0, 2.5, 3.0]\n",
    "        }\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for architecture in [\"monogenic\", \"oligogenic\"]:\n",
    "        for trait_freq in trait_frequencies:\n",
    "            for effect_size in effect_sizes[architecture]:\n",
    "                for sample_size in [50, 100, 200, 500, 1000]:\n",
    "                    n_cases = int(sample_size * trait_freq)\n",
    "                    n_controls = sample_size - n_cases\n",
    "                    \n",
    "                    # Skip if too few cases or controls\n",
    "                    if n_cases < 5 or n_controls < 5:\n",
    "                        continue\n",
    "                    \n",
    "                    # Create parameter hash\n",
    "                    param_hash = create_parameter_hash(\n",
    "                        architecture, trait_freq, effect_size, sample_size, n_replicates\n",
    "                    )\n",
    "                    \n",
    "                    # Check if simulation already exists\n",
    "                    if check_simulation_exists(output_path, param_hash):\n",
    "                        print(f\"Loading cached {architecture}, OR={effect_size}, trait_freq={trait_freq:.1f}, N={sample_size} ({param_hash})\")\n",
    "                        cached_result = load_existing_simulation(output_path, param_hash)\n",
    "                        \n",
    "                        # Extract summary for combined results\n",
    "                        summary_result = {\n",
    "                            'architecture': architecture,\n",
    "                            'trait_frequency': trait_freq,\n",
    "                            'effect_size_OR': effect_size,\n",
    "                            'sample_size': sample_size,\n",
    "                            'n_cases': n_cases,\n",
    "                            'n_controls': n_controls,\n",
    "                            'mean_power': cached_result['mean_power'],\n",
    "                            'se_power': cached_result['se_power'],\n",
    "                            'n_replicates': cached_result['n_replicates'],\n",
    "                            'param_hash': param_hash\n",
    "                        }\n",
    "                        all_results.append(summary_result)\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"Running {architecture}, OR={effect_size}, trait_freq={trait_freq:.1f}, N={sample_size} ({n_cases} cases, {n_controls} controls)...\")\n",
    "                    \n",
    "                    powers = []\n",
    "                    \n",
    "                    # Run replicates\n",
    "                    for replicate in range(n_replicates):\n",
    "                        print(f\"  Replicate {replicate}\")\n",
    "\n",
    "                        # Generate new population each time\n",
    "                        simulated_genos = simulate_genotypes(wp_maf, sample_size)\n",
    "                        causal_snp_indices, causal_effects = setup_causal_architecture(\n",
    "                            architecture, wp_maf, effect_size\n",
    "                        )\n",
    "                        \n",
    "                        # Calculate risk scores\n",
    "                        risk_scores = np.zeros(sample_size)\n",
    "                        for i, snp_idx in enumerate(causal_snp_indices):\n",
    "                            risk_scores += causal_effects[i] * simulated_genos[:, snp_idx]\n",
    "                        \n",
    "                        # Create cases/controls based on trait frequency\n",
    "                        binary_trait = create_balanced_cases_controls(risk_scores, n_cases)\n",
    "                        \n",
    "                        # Run GWAS\n",
    "                        p_values = run_gwas(simulated_genos, binary_trait)\n",
    "                        \n",
    "                        # Check power\n",
    "                        detected = np.sum(p_values[causal_snp_indices] < 5e-8)\n",
    "                        power = detected / len(causal_snp_indices)\n",
    "                        powers.append(power)\n",
    "                    \n",
    "                    mean_power = np.mean(powers)\n",
    "                    se_power = np.std(powers) / np.sqrt(len(powers))\n",
    "                    \n",
    "                    # Create result object\n",
    "                    simulation_result = {\n",
    "                        'architecture': architecture,\n",
    "                        'trait_frequency': trait_freq,\n",
    "                        'effect_size_OR': effect_size,\n",
    "                        'sample_size': sample_size,\n",
    "                        'n_cases': n_cases,\n",
    "                        'n_controls': n_controls,\n",
    "                        'mean_power': mean_power,\n",
    "                        'se_power': se_power,\n",
    "                        'n_replicates': len(powers),\n",
    "                        'power_values': powers,\n",
    "                        'param_hash': param_hash\n",
    "                    }\n",
    "                    \n",
    "                    # Save simulation result\n",
    "                    save_simulation_result(simulation_result, output_path, param_hash)\n",
    "                    \n",
    "                    # Add to summary results\n",
    "                    all_results.append(simulation_result)\n",
    "                    print(f\"  Power = {mean_power:.3f} ± {se_power:.3f} (saved as {param_hash})\")\n",
    "    \n",
    "    # Save combined results\n",
    "    global_hash = hashlib.md5(str(len(all_results)).encode()).hexdigest()[:8]\n",
    "    df = pd.DataFrame(all_results)\n",
    "    csv_file = output_path / f\"power_results_{global_hash}.csv\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Combined results saved to {csv_file}\")\n",
    "    \n",
    "    return all_results, df\n",
    "\n",
    "def load_simulation_result(output_dir, param_hash):\n",
    "    \"\"\"Load a specific simulation result by parameter hash\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    pickle_file = output_path / f\"sim_{param_hash}.pkl\"\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Plotting functions\n",
    "def plot_power_results(df, save_path=None):\n",
    "    \"\"\"Plot power results\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    for i, arch in enumerate([\"monogenic\", \"oligogenic\"]):\n",
    "        arch_data = df[df['architecture'] == arch]\n",
    "        \n",
    "        for effect_size in sorted(arch_data['effect_size_OR'].unique()):\n",
    "            effect_data = arch_data[arch_data['effect_size_OR'] == effect_size]\n",
    "            \n",
    "            for trait_freq in sorted(effect_data['trait_frequency'].unique()):\n",
    "                freq_data = effect_data[effect_data['trait_frequency'] == trait_freq]\n",
    "                \n",
    "                axes[i].plot(\n",
    "                    freq_data['sample_size'], \n",
    "                    freq_data['mean_power'],\n",
    "                    marker='o', \n",
    "                    label=f'OR={effect_size}, freq={trait_freq}',\n",
    "                    alpha=0.7\n",
    "                )\n",
    "        \n",
    "        axes[i].set_title(f'{arch.title()} Architecture')\n",
    "        axes[i].set_xlabel('Sample Size')\n",
    "        axes[i].set_ylabel('Power')\n",
    "        axes[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_power_heatmap(df, architecture=\"monogenic\", save_path=None):\n",
    "    \"\"\"Create heatmap of power by trait frequency vs effect size\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    arch_data = df[(df['architecture'] == architecture) & (df['sample_size'] == 200)]\n",
    "    \n",
    "    heatmap_data = arch_data.pivot(\n",
    "        index='trait_frequency', \n",
    "        columns='effect_size_OR', \n",
    "        values='mean_power'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(\n",
    "        heatmap_data, \n",
    "        annot=True, \n",
    "        fmt='.2f', \n",
    "        cmap='viridis',\n",
    "        cbar_kws={'label': 'Power'}\n",
    "    )\n",
    "    plt.title(f'{architecture.title()} Power (N=200)')\n",
    "    plt.xlabel('Effect Size (OR)')\n",
    "    plt.ylabel('Trait Frequency')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Heatmap saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "489d00cf-6ec4-4fbd-9d96-37bb18eb7259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Run analysis with default parameters (will skip cached simulations)\\nresults, df = power_analysis()\\n\\n# Run analysis with custom parameters\\nresults, df = power_analysis(\\n    trait_frequencies=[0.2, 0.5], \\n    effect_sizes={\"monogenic\": [7], \"oligogenic\": [2.0]},\\n    n_replicates=5\\n)\\n\\n# Run with different output directory\\nresults, df = power_analysis(\\n    output_dir=\"my_gwas_results\",\\n    trait_frequencies=[0.1, 0.3, 0.5],\\n    effect_sizes={\"monogenic\": [3, 5, 10], \"oligogenic\": [1.5, 2.5]},\\n    n_replicates=20\\n)\\n\\n# Load a specific simulation by hash\\nresult = load_simulation_result(\"gwas_results\", \"abc12345\")\\n\\n# Plot results\\nplot_power_results(df)\\nplot_power_heatmap(df, architecture=\"monogenic\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "\"\"\"\n",
    "# Run analysis with default parameters (will skip cached simulations)\n",
    "results, df = power_analysis()\n",
    "\n",
    "# Run analysis with custom parameters\n",
    "results, df = power_analysis(\n",
    "    trait_frequencies=[0.2, 0.5], \n",
    "    effect_sizes={\"monogenic\": [7], \"oligogenic\": [2.0]},\n",
    "    n_replicates=5\n",
    ")\n",
    "\n",
    "# Run with different output directory\n",
    "results, df = power_analysis(\n",
    "    output_dir=\"my_gwas_results\",\n",
    "    trait_frequencies=[0.1, 0.3, 0.5],\n",
    "    effect_sizes={\"monogenic\": [3, 5, 10], \"oligogenic\": [1.5, 2.5]},\n",
    "    n_replicates=20\n",
    ")\n",
    "\n",
    "# Load a specific simulation by hash\n",
    "result = load_simulation_result(\"gwas_results\", \"abc12345\")\n",
    "\n",
    "# Plot results\n",
    "plot_power_results(df)\n",
    "plot_power_heatmap(df, architecture=\"monogenic\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca4869e-2928-4b94-9c9a-acda012f6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached monogenic, OR=1, trait_freq=0.2, N=50 (bd400ae4)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.2, N=100 (2f25a4b2)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.2, N=200 (f544545e)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.2, N=500 (3ab267be)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.2, N=1000 (d8fcf3ad)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.2, N=50 (aad1a2da)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.2, N=100 (688ee4e8)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.2, N=200 (a4856db7)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.2, N=500 (c425a95b)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.2, N=1000 (f09262ec)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.2, N=50 (ecaf991d)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.2, N=100 (ada34ec3)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.2, N=200 (0b61b5ef)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.2, N=500 (d3ae7489)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.2, N=1000 (57ce8f91)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.2, N=50 (acd0a52b)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.2, N=100 (c4539d1b)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.2, N=200 (36b23a5e)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.2, N=500 (85dc2566)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.2, N=1000 (08dc8bf0)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.2, N=50 (bdc93785)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.2, N=100 (bb362a30)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.2, N=200 (cc46cd45)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.2, N=500 (e224a8ab)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.2, N=1000 (eb8306b0)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.5, N=50 (a73286f3)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.5, N=100 (d390de86)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.5, N=200 (f6543980)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.5, N=500 (9ed14e30)\n",
      "Loading cached monogenic, OR=1, trait_freq=0.5, N=1000 (5e8b3c84)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.5, N=50 (30d8a23b)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.5, N=100 (573d246b)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.5, N=200 (e54bf977)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.5, N=500 (7f1a551c)\n",
      "Loading cached monogenic, OR=3, trait_freq=0.5, N=1000 (ccae862c)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.5, N=50 (5ad6afed)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.5, N=100 (9fb534d1)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.5, N=200 (8e08c0aa)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.5, N=500 (ed174313)\n",
      "Loading cached monogenic, OR=5, trait_freq=0.5, N=1000 (262d887a)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.5, N=50 (bed59583)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.5, N=100 (61db8db8)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.5, N=200 (8f279898)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.5, N=500 (3ac5278e)\n",
      "Loading cached monogenic, OR=7, trait_freq=0.5, N=1000 (aaae0f0a)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.5, N=50 (63231b04)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.5, N=100 (4b607f41)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.5, N=200 (be7a418d)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.5, N=500 (2852fa29)\n",
      "Loading cached monogenic, OR=10, trait_freq=0.5, N=1000 (4340bb1e)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.2, N=50 (a29686e9)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.2, N=100 (85db33d9)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.2, N=200 (347e7dda)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.2, N=500 (0c938606)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.2, N=1000 (fa47cd8a)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.2, N=50 (22f211be)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.2, N=100 (cd03f5a0)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.2, N=200 (6508c8f9)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.2, N=500 (e9d386e8)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.2, N=1000 (e6d34ec7)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.2, N=50 (b6db4b06)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.2, N=100 (1ef7d440)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.2, N=200 (97a56f22)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.2, N=500 (dd937b0a)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.2, N=1000 (98a3e457)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.5, N=50 (427a93c5)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.5, N=100 (88ae4c99)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.5, N=200 (42ccd70c)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.5, N=500 (4fe3cc88)\n",
      "Loading cached oligogenic, OR=1.5, trait_freq=0.5, N=1000 (433c4368)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.5, N=50 (9de9077a)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.5, N=100 (dc8da1d1)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.5, N=200 (b0336f8b)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.5, N=500 (35bc6c30)\n",
      "Loading cached oligogenic, OR=2, trait_freq=0.5, N=1000 (617107c5)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.5, N=50 (89d7820d)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.5, N=100 (cc62d9dd)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.5, N=200 (81a29a7b)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.5, N=500 (8064cc61)\n",
      "Loading cached oligogenic, OR=2.5, trait_freq=0.5, N=1000 (9760cc39)\n",
      "Combined results saved to gwas_results/power_results_f033ab37.csv\n"
     ]
    }
   ],
   "source": [
    "results, df = power_analysis(\n",
    "    trait_frequencies=[0.2, 0.5], \n",
    "    effect_sizes={\"monogenic\": [1, 3, 5, 7, 10], \"oligogenic\": [1.5, 2, 2.5]},\n",
    "    n_replicates=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114bc20-5c88-4c00-9acc-8b17081c8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO - get this to run in parallel or in dask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "far_hin_1.x",
   "language": "python",
   "name": "far_hin_1.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
