{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511396c7-00bf-43a0-a016-bd7e5711c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import xarray as xr\n",
    "import sgkit as sg\n",
    "import pandas as pd\n",
    "import numcodecs\n",
    "from Bio import SeqIO\n",
    "from fsspec.implementations.zip import ZipFileSystem\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "# Classes\n",
    "try:\n",
    "    \n",
    "    # newer versions of zarr\n",
    "    \n",
    "    from zarr.storage import KVStore\n",
    "    \n",
    "    class SafeStore(KVStore):\n",
    "        \n",
    "        def __getitem__(self, key):\n",
    "            try:\n",
    "                return self._mutable_mapping[key]\n",
    "            except KeyError as e:\n",
    "                # always raise a runtime error to ensure zarr propagates the exception\n",
    "                raise RuntimeError(e)\n",
    "\n",
    "        def __contains__(self, key):\n",
    "            return key in self._mutable_mapping\n",
    "\n",
    "                \n",
    "except ImportError:\n",
    "    \n",
    "    # older versions of zarr\n",
    "\n",
    "    class SafeStore(Mapping):\n",
    "\n",
    "        ## This helps to ensure that no missing data are silently filled in.\n",
    "\n",
    "        def __init__(self, store):\n",
    "            self.store = store\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        try:\n",
    "            return self.store[key]\n",
    "        except KeyError as e:\n",
    "            # Allow missing .zattrs to return empty JSON for attribute reading\n",
    "            if key.endswith(\".zattrs\"):\n",
    "                return b'{}'\n",
    "            raise RuntimeError(f\"Missing key in Zarr store: {key}\") from e\n",
    "\n",
    "        def __contains__(self, key):\n",
    "            return key in self.store\n",
    "\n",
    "        def __iter__(self):\n",
    "            return iter(self.store)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a7e813-bcc9-4a61-8fd3-ca437f0a7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get started\n",
    "# Load paths and metadata\n",
    "ref_path = '/QRISdata/Q6151/dennistpw/far_hin_1.x/data/reference/VectorBase-54_AfarautiFAR1_Genome.fasta'\n",
    "output_dir_path = f'/scratch/user/uqtdenni/snp_genotypes_combined.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c114951d-c543-4d9d-ac74-7712ba08b0b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "'KI915040/call_genotype/.zattrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mSafeStore.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mutable_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# always raise a runtime error to ensure zarr propagates the exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/zarr/storage.py:1120\u001b[39m, in \u001b[36mDirectoryStore.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1120\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'KI915040/call_genotype/.zattrs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 221\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcall_genotype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcall_AD\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcall_GQ\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m seq_id \u001b[38;5;129;01min\u001b[39;00m called_contigs:\n\u001b[32m    216\u001b[39m \n\u001b[32m    217\u001b[39m         \u001b[38;5;66;03m# try this to clear cluster memory between arrays\u001b[39;00m\n\u001b[32m    218\u001b[39m         \u001b[38;5;66;03m# client.restart()\u001b[39;00m\n\u001b[32m    219\u001b[39m \n\u001b[32m    220\u001b[39m         \u001b[38;5;66;03m# Combine the source data for each sample in the sample set into one output zarr\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m         \u001b[43mcombine_zarr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m             \u001b[49m\u001b[43mseq_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m             \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 182\u001b[39m, in \u001b[36mcombine_zarr\u001b[39m\u001b[34m(seq_id, field, force)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcombine_zarr\u001b[39m(seq_id, field, force=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    180\u001b[39m \n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# Get the complete-ness status of the output array, along with the output array itself.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     complete, output_array, gcs_output_store = \u001b[43msetup_output_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# If the output array is already complete and the option to force its replacement is not set\u001b[39;00m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m complete \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force:\n\u001b[32m    190\u001b[39m \n\u001b[32m    191\u001b[39m         \u001b[38;5;66;03m# Don't copy over data for this sample set, because it's already complete and we shouldn't replace\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36msetup_output_array\u001b[39m\u001b[34m(seq_id, field, cname, clevel, shuffle, chunk_height, chunk_width, force)\u001b[39m\n\u001b[32m     91\u001b[39m output_arr = seq_group[field]\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Get the value of the \"complete\" flag, or default to False (perhaps unnecessary but explicit)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m complete = \u001b[43moutput_arr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# If the data is marked complete and the force flag is not set\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m complete \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force:\n\u001b[32m     98\u001b[39m \n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Return the state of the complete flag (True) and the existing data array\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/_collections_abc.py:811\u001b[39m, in \u001b[36mMapping.get\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[33m'\u001b[39m\u001b[33mD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/zarr/attrs.py:74\u001b[39m, in \u001b[36mAttributes.__getitem__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[item]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/zarr/attrs.py:55\u001b[39m, in \u001b[36mAttributes.asdict\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_asdict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_asdict\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m d = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_nosync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._version == \u001b[32m3\u001b[39m:\n\u001b[32m     57\u001b[39m     d = d[\u001b[33m\"\u001b[39m\u001b[33mattributes\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/zarr/attrs.py:42\u001b[39m, in \u001b[36mAttributes._get_nosync\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_nosync\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m     44\u001b[39m         d = \u001b[38;5;28mdict\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mSafeStore.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mutable_mapping[key]\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# always raise a runtime error to ensure zarr propagates the exception\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(e)\n",
      "\u001b[31mRuntimeError\u001b[39m: 'KI915040/call_genotype/.zattrs'"
     ]
    }
   ],
   "source": [
    "# Functions\n",
    "def get_contigs(ref_path, length_threshold=1e5):\n",
    "    # now let's get a list of the contigs that we are going to call over\n",
    "    contig_lengths = {}\n",
    "    for record in SeqIO.parse(ref_path, \"fasta\"):\n",
    "        seq_id = record.id\n",
    "        seq_length = len(record.seq)\n",
    "        contig_lengths[seq_id] = seq_length\n",
    "    \n",
    "    # Filter to called contigs (over 1e5 bp)\n",
    "    return {k: v for k, v in sorted(contig_lengths.items(), key=lambda item: item[1], reverse=True) if v > length_threshold}\n",
    "\n",
    "def load_samples():\n",
    "    \n",
    "    ## Returns the sorted unique list of samples from the sample manifest\n",
    "\n",
    "    # Read the samples from the file\n",
    "    fofn = pd.read_table('/home/uqtdenni/far_hin_1.x/metadata/fofn.tsv')\n",
    "\n",
    "    # Get the unique list of samples from the DataFrame and sort\n",
    "    samples = fofn['zarr_name'].dropna().unique().tolist()\n",
    "    return samples\n",
    "\n",
    "def open_input(sample, seq_id, field):\n",
    "\n",
    "    zip_file = f\"/scratch/user/uqtdenni/snp_genotypes_zarr/{sample}.zarr.zip\"\n",
    "    \n",
    "    zip_fs = ZipFileSystem(zip_file)\n",
    "\n",
    "    # Map the full path to one contig\n",
    "    contig_path = f\"{sample}.zarr/{seq_id}\"\n",
    "    \n",
    "    zarr_store = zip_fs.get_mapper(contig_path)\n",
    "\n",
    "    z = zarr.open(zarr_store)\n",
    "\n",
    "    #z = zarr_store[f\"{field}\"]\n",
    "\n",
    "    #zarr_path = zarr_path_template.format(sample_id = s)\n",
    "    #array_path = f'{sample}.zarr/{seq_id}/{field}'\n",
    "\n",
    "    # open and load the array\n",
    "    #root =  zarr.ZipStore(zarr_path)\n",
    "    field = z[f'{field}'][:]\n",
    "\n",
    "    return field\n",
    "def setup_output_array(seq_id, field,  \n",
    "                       cname='zstd', clevel=1, shuffle=-1, \n",
    "                       chunk_height=1000000, chunk_width=10, force=True): \n",
    "    \n",
    "    ## Returns the complete-ness status of the data array at the specified output path,\n",
    "    ##   along with the array itself and the gcs_output_store (for consolidating metadata).\n",
    "    ## The sample_set, seq_id and field determine the output path according to the path template `output_pattern`.\n",
    "    ## If a complete array does not yet exist at the specfied path, the example_arr is used to determine the shape, dtype and chunks of the output array.\n",
    "    ## The length of `samples` is also used to determine the shape of the output array.\n",
    "    ## The specfied or default chunk_height and chunk_width are also used to determine the output chunks.\n",
    "    \n",
    "    ##  https://numcodecs.readthedocs.io/en/stable/blosc.html\n",
    "    ## `clevel` is the compression level for the compressor, i.e. numcodecs.Blosc, between 0 and 9\n",
    "    ## `shuffle` is the shuffle option for the compressor, where -1 means autoshuffle, either bit- or byte-shuffle\n",
    "    \n",
    "    ## The `force` option causes existing data marked as `complete` to be overwritten, initially with zeros (as with incomplete data)\n",
    "            \n",
    "    # Wrap the local DirectoryStore with SafeStore\n",
    "    local_store = SafeStore(zarr.DirectoryStore(output_dir_path))\n",
    "    \n",
    "    # Open or create the Zarr group using the safe store\n",
    "    output_zarr_root = zarr.open_group(store=local_store, mode='a')\n",
    "        \n",
    "    ## Create the output subgroups, unless they already exist\n",
    "    \n",
    "    # Make a zarr subgroup of the root, named after the seq_id, e.g. \"2L\"\n",
    "    seq_group = output_zarr_root.require_group(seq_id)\n",
    "    \n",
    "    # Make a zarr subgroup of the seq_group, named \"calldata\", e.g. \"2L\" > \"calldata\"\n",
    "    #calldata_group = seq_group.require_group(\"calldata\")\n",
    "    \n",
    "    ## Stay DRY\n",
    "    \n",
    "    # Get the field_name from the field string, e.g. 'AD' from 'calldata/AD', 'MQ' from 'variants/MQ'\n",
    "    field_name = field#split(\"_\")[1]\n",
    "    \n",
    "    # Set the completion flag to false\n",
    "    # This mechanism will allow us to re-run the process to only create data that hasn't been flagged \"complete\"\n",
    "    complete = False\n",
    "    \n",
    "    # If the field_name is already in the \"calldata\" zarr subgroup \n",
    "    if field_name in seq_group:\n",
    "        \n",
    "        # Get the data array for this field_name\n",
    "        output_arr = seq_group[field]\n",
    "        \n",
    "        # Get the value of the \"complete\" flag, or default to False (perhaps unnecessary but explicit)\n",
    "        complete = output_arr.attrs.get(\"complete\", False)\n",
    "        \n",
    "        # If the data is marked complete and the force flag is not set\n",
    "        if complete and not force:\n",
    "            \n",
    "            # Return the state of the complete flag (True) and the existing data array\n",
    "            return complete, output_arr, gcs_output_store\n",
    "    \n",
    "    ## Determine output array characteristics\n",
    "    \n",
    "    # The output_shape should have the same number of rows as the input data shape,\n",
    "    # and have the same number of columns as there are samples.\n",
    "    # The other dimensions of the output_shape should should match the other dimensions in the input data.\n",
    "    input_dtype, input_shape = check_array_setup(seq_id=seq_id, field=field)\n",
    "    output_shape = (input_shape[0], len(samples)) + input_shape[2:]\n",
    "    \n",
    "    # The output data type should be the same as the input data type.\n",
    "    output_dtype = input_dtype\n",
    "    \n",
    "    # Specify an appropriate chunk size.\n",
    "    # Why this number? Big enough to give reasonable chunk sizes.\n",
    "    target_chunk_size = 2**26\n",
    "    \n",
    "    # The output chunks should have the specified height and width,\n",
    "    #   and have the same other chunk dimensions as the input data.\n",
    "    output_chunks = (chunk_height, chunk_width) + output_shape[2:]\n",
    "    \n",
    "    # Choose a compressor for the output data\n",
    "    compressor = numcodecs.Blosc(cname=cname, clevel=clevel, shuffle=shuffle)\n",
    "    \n",
    "    # Create an output array of zeros named after the field_name, with the determined shape, dtype and chunks.\n",
    "    output_arr = seq_group.zeros(field_name, shape=output_shape, dtype=output_dtype,\n",
    "                                      chunks=output_chunks, overwrite=True, compressor=compressor)\n",
    "    \n",
    "    # Return the complete-ness status of the output array, along with the output array itself (either blank or as it exists)\n",
    "    return complete, output_arr, local_store\n",
    "\n",
    "def check_array_setup(seq_id, field):\n",
    "    \n",
    "    # Returns information about data array for the first sample for the specified \n",
    "    # sample set, seq_id and field.\n",
    "\n",
    "    z = open_input(sample=samples[0], seq_id=seq_id, field=field)\n",
    "    return z.dtype, z.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def setup_input_array(seq_id, field):\n",
    "    \n",
    "    input_arrays = []\n",
    "    dtype, shape = check_array_setup(seq_id=seq_id, field=field)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        \n",
    "        # N.B., here we use dask delayed so we avoid opening any zipped zarr\n",
    "        # files in the main notebook process, rather opening will only happen\n",
    "        # within the workers. This should hopefully avoid running out of memory\n",
    "        # in the main notebook process.\n",
    "        z = dask.delayed(open_input)(sample=sample, seq_id=seq_id, field=field)\n",
    "        a = da.from_delayed(z, shape=shape, dtype=dtype)\n",
    "        \n",
    "        if a.ndim == 1:\n",
    "            # add a dimension to allow for concatenation along samples axis\n",
    "            a = a[:, None]\n",
    "        input_arrays.append(a)\n",
    "            \n",
    "    input_array = da.concatenate(input_arrays, axis=1)\n",
    "    \n",
    "    return input_array\n",
    "\n",
    "def copy_data(input_arr, output_arr):\n",
    "    \n",
    "    # Convert the input array to the same chunks as the output array\n",
    "    input_arr = input_arr.rechunk(output_arr.chunks)\n",
    "    \n",
    "    # Store the input array in the output array.\n",
    "    # Don't lock the data stores while storing.\n",
    "    # https://docs.dask.org/en/latest/array-api.html#dask.array.store\n",
    "    input_arr.store(output_arr, lock=False)\n",
    "    \n",
    "    # Set the \"complete\" attribute on the output array to True, to mark completion\n",
    "    output_arr.attrs['complete'] = True\n",
    "\n",
    "\n",
    "def combine_zarr(seq_id, field, force=False):\n",
    "\n",
    "    # Get the complete-ness status of the output array, along with the output array itself.\n",
    "    complete, output_array, gcs_output_store = setup_output_array(\n",
    "        seq_id=seq_id,\n",
    "        field=field,\n",
    "        force=force\n",
    "    )\n",
    "    \n",
    "    # If the output array is already complete and the option to force its replacement is not set\n",
    "    if complete and not force:\n",
    "        \n",
    "        # Don't copy over data for this sample set, because it's already complete and we shouldn't replace\n",
    "        print('output exists, skipping', seq_id, field)\n",
    "        return\n",
    "    \n",
    "    print('processing', seq_id, field)\n",
    "    \n",
    "    # Get the input array for this sample set, for this seq_id and field.\n",
    "    input_array = setup_input_array(seq_id=seq_id, field=field)\n",
    "\n",
    "    print(f\"input chunks: {input_array.chunks}\")\n",
    "    print(f\"output chunks: {output_array.chunks}\")\n",
    "    \n",
    "    # Copy the input data (for the specfied samples) to the output array (for the specfied sample set)\n",
    "    copy_data(input_array, output_array)\n",
    "\n",
    "called_contigs = get_contigs(ref_path)\n",
    "samples = load_samples()\n",
    "\n",
    "#input_dir = Path(\"/scratch/user/uqtdenni/snp_genotypes_zarr/\")\n",
    "#input_zarr_paths = sorted(input_dir.glob(\"*.zarr\"))  # Adjust pattern if needed\n",
    "\n",
    "nsamples = len(samples)\n",
    "\n",
    "for field in 'call_genotype', 'call_AD', 'call_GQ':\n",
    "    for seq_id in called_contigs:\n",
    "\n",
    "        # try this to clear cluster memory between arrays\n",
    "        # client.restart()\n",
    "        \n",
    "        # Combine the source data for each sample in the sample set into one output zarr\n",
    "        combine_zarr(\n",
    "             seq_id=seq_id,\n",
    "             field=field,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "331cd107-a72c-4709-b340-33a2e3c7d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing contig: KI915083\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/user/uqtdenni/snp_genotypes_zarr//scratch/user/uqtdenni/snp_genotypes_zarr/tor_wQLD_Por_13.zarr.zip.zarr.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m datasets = []\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     ds = \u001b[43mopen_sample_contig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontig\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# one-sample dataset\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Confirm it has dimensions: variants, ploidy (and probably samples = 1)\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ds.dims:\n\u001b[32m     43\u001b[39m         \u001b[38;5;66;03m# Add sample dimension manually if not present\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mopen_sample_contig\u001b[39m\u001b[34m(sample, contig)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_sample_contig\u001b[39m(sample, contig):\n\u001b[32m      7\u001b[39m     zip_file = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/scratch/user/uqtdenni/snp_genotypes_zarr/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.zarr.zip\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     zip_fs = \u001b[43mZipFileSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Map the full path to one contig\u001b[39;00m\n\u001b[32m     12\u001b[39m     contig_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.zarr/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontig\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/fsspec/spec.py:81\u001b[39m, in \u001b[36m_Cached.__call__\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._cache[token]\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     obj = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Setting _fs_token here causes some static linters to complain.\u001b[39;00m\n\u001b[32m     83\u001b[39m     obj._fs_token_ = token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/fsspec/implementations/zip.py:62\u001b[39m, in \u001b[36mZipFileSystem.__init__\u001b[39m\u001b[34m(self, fo, mode, target_protocol, target_options, compression, allowZip64, compresslevel, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.force_zip_64 = allowZip64\n\u001b[32m     61\u001b[39m \u001b[38;5;28mself\u001b[39m.of = fo\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28mself\u001b[39m.fo = \u001b[43mfo\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# the whole instance is a context\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.zip = zipfile.ZipFile(\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m.fo,\n\u001b[32m     65\u001b[39m     mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m     compresslevel=compresslevel,\n\u001b[32m     69\u001b[39m )\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m.dir_cache = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/fsspec/core.py:105\u001b[39m, in \u001b[36mOpenFile.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    102\u001b[39m mode = \u001b[38;5;28mself\u001b[39m.mode.replace(\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_magic(\u001b[38;5;28mself\u001b[39m.path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/fsspec/spec.py:1338\u001b[39m, in \u001b[36mAbstractFileSystem.open\u001b[39m\u001b[34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[39m\n\u001b[32m   1336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1337\u001b[39m     ac = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mautocommit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._intrans)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1347\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfsspec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/fsspec/implementations/local.py:206\u001b[39m, in \u001b[36mLocalFileSystem._open\u001b[39m\u001b[34m(self, path, mode, block_size, **kwargs)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedirs(\u001b[38;5;28mself\u001b[39m._parent(path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/fsspec/implementations/local.py:383\u001b[39m, in \u001b[36mLocalFileOpener.__init__\u001b[39m\u001b[34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28mself\u001b[39m.compression = get_compression(path, compression)\n\u001b[32m    382\u001b[39m \u001b[38;5;28mself\u001b[39m.blocksize = io.DEFAULT_BUFFER_SIZE\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/far_hin_1.x/lib/python3.13/site-packages/fsspec/implementations/local.py:388\u001b[39m, in \u001b[36mLocalFileOpener._open\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f.closed:\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.autocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         \u001b[38;5;28mself\u001b[39m.f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compression:\n\u001b[32m    390\u001b[39m             compress = compr[\u001b[38;5;28mself\u001b[39m.compression]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/scratch/user/uqtdenni/snp_genotypes_zarr//scratch/user/uqtdenni/snp_genotypes_zarr/tor_wQLD_Por_13.zarr.zip.zarr.zip'"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "import fsspec\n",
    "import numpy as np\n",
    "\n",
    "# Your open_input adapted to return the whole zarr group for a contig lazily:\n",
    "def open_sample_contig(sample, contig):\n",
    "    zip_file = f\"/scratch/user/uqtdenni/snp_genotypes_zarr/{sample}.zarr.zip\"\n",
    "    \n",
    "    zip_fs = ZipFileSystem(zip_file)\n",
    "\n",
    "    # Map the full path to one contig\n",
    "    contig_path = f\"{sample}.zarr/{contig}\"\n",
    "    \n",
    "    zarr_store = zip_fs.get_mapper(contig_path)\n",
    "\n",
    "    z = xr.open_zarr(zarr_store, consolidated=False)\n",
    "\n",
    "    \n",
    "    return z\n",
    "\n",
    "called_contigs = get_contigs(ref_path)\n",
    "samples = load_samples()\n",
    "\n",
    "# The field you want to merge (example)\n",
    "\n",
    "# Dictionary to hold merged contig arrays\n",
    "merged_contigs = {}\n",
    "\n",
    "for contig in called_contigs:\n",
    "    print(f\"Processing contig: {contig}\")\n",
    "    # Open arrays for this contig from all samples\n",
    "    datasets = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        ds = open_sample_contig(sample, contig)  # one-sample dataset\n",
    "    \n",
    "        # Confirm it has dimensions: variants, ploidy (and probably samples = 1)\n",
    "        if \"samples\" not in ds.dims:\n",
    "            # Add sample dimension manually if not present\n",
    "            ds = ds.expand_dims(\"samples\")\n",
    "    \n",
    "        else:\n",
    "            # If samples dim exists but is size 1, make sure it's handled correctly\n",
    "            ds = ds.isel(samples=0).expand_dims(\"samples\")\n",
    "    \n",
    "        datasets.append(ds)\n",
    "\n",
    "# Concatenate along the 'samples' dimension\n",
    "merged = xr.concat(datasets, dim=\"samples\")\n",
    "\n",
    "# Now merged_contigs contains one concatenated array per contig,\n",
    "# stacked with shape (nsamples, nsites, ...) depending on your data.\n",
    "\n",
    "# Example: save merged contigs to disk or do further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ca26bed-7c1b-44a9-86aa-fa15e577b5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/user/uqtdenni/snp_genotypes_zarr/tor_wQLD_Por_13.zarr.zip',\n",
       " '/scratch/user/uqtdenni/snp_genotypes_zarr/ore_IJ98_12_5.zarr.zip',\n",
       " '/scratch/user/uqtdenni/snp_genotypes_zarr/pun_SI98-12-2.zarr.zip']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bff2d6a1-f491-4662-8fcc-08b122dd4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#41s for a million bp\n",
    "%load_ext memory_magics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c16664-5235-4417-b1a3-ddb54edb7e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "far_hin_1.x",
   "language": "python",
   "name": "far_hin_1.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
